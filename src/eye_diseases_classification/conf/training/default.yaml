# Training configuration

epochs: 30
batch_size: 64

# Optimizer
optimizer: "adamw"
learning_rate: 0.001
weight_decay: 0.0001

# Learning rate scheduler
scheduler:
  enabled: true
  type: "reduce_on_plateau"  # reduce_on_plateau, cosine, step
  factor: 0.5
  patience: 3
  min_lr: 0.00001

# Early stopping
early_stopping:
  enabled: true
  monitor: "val_loss"
  patience: 10
  mode: "min"

# Checkpointing
checkpoint:
  monitor: "val_acc"
  mode: "max"
  save_top_k: 1
  filename: "best_model-{epoch:02d}-{val_acc:.3f}"
