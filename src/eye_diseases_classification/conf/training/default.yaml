# Training configuration

epochs: 150
batch_size: 64

# Optimizer
optimizer: "adamw"
learning_rate: 0.0003
weight_decay: 0.001

# Learning rate scheduler
scheduler:
  enabled: true
  type: "cosine"  
  min_lr: 0.00001

# Early stopping
early_stopping:
  enabled: true
  monitor: "val_loss"
  patience: 15
  mode: "min"

# Checkpointing
checkpoint:
  monitor: "val_acc"
  mode: "max"
  save_top_k: 1
  filename: "best_model-{epoch:02d}-{val_acc:.3f}"
